# Требования к частям системы (Subsystem Requirements)

## 1. Мобильный клиент iOS (SwiftUI) — `src/app/mad_application`

### 1.1. Presentation слой (UI/UX)

- UI‑1: Навигация должна быть предсказуемой (Tab/Stack), глубина сценариев ограничена.
- UI‑2: Основные экраны должны поддерживать состояние: loading/empty/content/error.
- UI‑3: Для ошибок сети — отдельное «человеческое» сообщение и действие (Retry).
- UI‑4: Доступность: VoiceOver labels для интерактивных элементов, Dynamic Type, контраст.
- UI‑5: Экран подключения поддерживает проверку доступности и отображение статуса.
- UI‑6: Экран чата поддерживает отправку и получение ответа (без streaming/stop).
- UI‑7: Настройки параметров генерации не реализованы (план).

### 1.2. Domain слой (Use Cases)

- D‑1: Все бизнес‑операции должны быть оформлены как use‑cases с явными входами/выходами.
- D‑2: Должны быть определены доменные модели без зависимости от UI/Network.
- D‑3: Ошибки должны быть типизированы (network/auth/validation/unknown) для UX‑обработки.
- D‑4: Use‑case «SendPrompt» отправляет промпт и получает ответ без streaming/cancel.
- D‑5: Use‑case «ListModels» должен получать список моделей Ollama (через gateway или напрямую).
- D‑6: Проверка подключения выполняется через запрос `/models`.

### 1.3. Data слой (Repositories)

- R‑1: Репозитории должны скрывать источник данных (remote/local/cache).
- R‑2: Кэш не реализован (запланировано).
- R‑3: Должна быть стратегия ретраев и таймаутов для сетевых запросов.
- R‑4: История диалогов не хранится.

### 1.4. Networking

- N‑1: Все запросы выполняются через единый клиент (URLSession), с логированием без PII.
- N‑2: Поддерживаются стандартные коды ошибок (401/403/404/429/5xx) и маппинг в доменные ошибки.
- N‑3: Конфигурация окружений (dev/stage/prod) задается через build settings или конфиг.
- N‑4: Streaming не используется.
- N‑5: Должны быть ограничения: максимальный размер промпта/контекста на клиенте, чтобы избежать OOM/тормозов.

### 1.5. Persistence (локальные данные)

- P‑1: Токены сейчас хранятся в `AppStorage` (план: Keychain).
- P‑2: Кэш данных — в CoreData/SQLite/FileCache (выбрать), с ограничением размера и TTL.
- P‑3: При очистке данных (logout) локальный кэш пользователя должен быть удален.
- P‑4: Профили подключения (адреса) должны храниться локально; секреты (ключи/токены) — раздельно в Keychain.

### 1.6. Observability в клиенте

- O‑1: Crash‑события не подключены (план: Crashlytics/Sentry).
- O‑2: Клиент логирует UX‑события и действия (тапы, смена состояния) в `/client-metrics`.
- O‑3: Клиент измеряет время загрузки моделей/ответа и длительность сессии.
- O‑4: TTFT/cancel rate не измеряются (streaming не используется).

### 1.7. Обратная связь в клиенте

- F‑1: Feedback prompt показывается после завершения чата.
- F‑2: Rate‑limit не используется, показ с задержкой 10 секунд.
- F‑3: Пользователь может приложить комментарий и (опционально) скриншот/логи без PII.

## 2. Backend API (Gateway/монолит или микросервисы)

### 2.1. Home Gateway / BFF (10.8.1.1)

- BFF‑1: Единая точка входа для мобильного клиента к домашнему ПК; проксирует запросы к Ollama.
- BFF‑2: Аутентификация клиента (token/pairing/mTLS) обязательна; анонимный доступ запрещен.
- BFF‑3: Rate limiting и защита от злоупотреблений (429), лимиты промпта/частоты запросов.
- BFF‑4: Логирование запросов и корреляция (request_id/trace_id), без сохранения промптов по умолчанию.
- BFF‑5: Streaming проксирование не используется.
- Практическая реализация: `src/home_gateway` (FastAPI, Dockerfile, rate limiting, bearer auth, `/health`, `/models`, `/chat`).

### 2.2. Auth (в составе gateway или отдельный сервис)

- AUTH‑1: Pairing устройства (одноразовый код/QR) и выпуск ключа/токена для клиента.
- AUTH‑2: Ротация/отзыв токенов и управление списком привязанных устройств.

### 2.3. Ollama service (уже установлен на домашнем ПК)

- OLL‑1: Предоставляет endpoint для генерации текста и список доступных моделей.
- OLL‑2: Должен быть доступен только из доверенной сети/через gateway (не «голый» в интернет).

### 2.4. Data storage (опционально)

- DB‑1: При необходимости истории/настроек на сервере — хранить только метаданные и конфиги (по умолчанию без промптов).
- DB‑2: Если сохраняются диалоги — включить явное согласие пользователя и шифрование.

## 3. Система качества (Observability/Monitoring)

### 3.1. Метрики и мониторинг

- MON‑1: Backend экспортирует метрики в Prometheus (`/metrics`).
- MON‑2: Grafana дашборд провизионится из файлов `src/grafana`.
- MON‑3: Zabbix не развернут (опционально).

### 3.2. Логи и трассировки

- LOG‑1: Централизованный сбор логов не реализован (план: Loki/ELK).
- TRACE‑1: Distributed tracing не реализован.

## 4. Аналитика продукта

- ANA‑1: События фиксируются с версией приложения, платформой, экраном, исходом (success/fail).
- ANA‑2: Не хранить PII без необходимости; обеспечить согласие пользователя при сборе.
- ANA‑3: Время сессии фиксируется (`session_duration`).
- ANA‑4: Промпт/ответ не отправляются в аналитику по умолчанию; допускается только агрегированная телеметрия.

## 5. Сервис обратной связи

- FB‑1: Прием NPS/CSI ответов и текстовых сообщений.
- FB‑2: Хранение: оценка, комментарий, сценарий, timestamp.
- FB‑3: Маршрутизация в issue tracker (опционально) и SLA обработки.

## 6. VPN слой (общая сеть 10.8.1.x)

- VPN‑1: Все устройства (телефон 10.8.1.2, gateway 10.8.1.1, Ollama 10.8.1.3) объединены в одну частную сеть; трафик не выходит в интернет в открытом виде.
- VPN‑2: Должны быть правила firewall, ограничивающие доступ к портам gateway/Ollama только из этой сети.
- VPN‑3: Мониторинг соединения VPN (uptime, latency) обязателен для диагностики качества.
